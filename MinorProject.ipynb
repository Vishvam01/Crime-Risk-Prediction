{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afccbf7-3dcc-4259-8f8f-54d0442822cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b62d41-09ca-4d8f-821c-70882e0b522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Report Number Date Reported Date of Occurrence Time of Occurrence  \\\n",
      "0              1      2/1/2020           1/1/2020            1:11:00   \n",
      "1              2      1/1/2020           2/1/2020            6:26:00   \n",
      "2              3      2/1/2020           3/1/2020           14:30:00   \n",
      "3              4      1/1/2020           4/1/2020           14:46:00   \n",
      "4              5      1/1/2020           5/1/2020           16:51:00   \n",
      "\n",
      "        City Crime Description  Victim Age Victim Gender   Weapon Used  \\\n",
      "0  Ahmedabad    IDENTITY THEFT          16             M  Blunt Object   \n",
      "1    Chennai          HOMICIDE          37             M        Poison   \n",
      "2   Ludhiana        KIDNAPPING          48             F  Blunt Object   \n",
      "3       Pune          BURGLARY          49             F       Firearm   \n",
      "4       Pune         VANDALISM          30             F         Other   \n",
      "\n",
      "  Case Closed  \n",
      "0          No  \n",
      "1          No  \n",
      "2          No  \n",
      "3         Yes  \n",
      "4         Yes  \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"crime_dataset_india.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c912a8-ac77-4efe-a9c6-29feb3f9a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40160 entries, 0 to 40159\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Report Number       40160 non-null  int64 \n",
      " 1   Date Reported       40160 non-null  object\n",
      " 2   Date of Occurrence  40160 non-null  object\n",
      " 3   Time of Occurrence  40160 non-null  object\n",
      " 4   City                40160 non-null  object\n",
      " 5   Crime Description   40160 non-null  object\n",
      " 6   Victim Age          40160 non-null  int64 \n",
      " 7   Victim Gender       40160 non-null  object\n",
      " 8   Weapon Used         34370 non-null  object\n",
      " 9   Case Closed         40160 non-null  object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 3.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce123d8d-0754-4cc8-815d-0671d965c20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Number            0\n",
      "Date Reported            0\n",
      "Date of Occurrence       0\n",
      "Time of Occurrence       0\n",
      "City                     0\n",
      "Crime Description        0\n",
      "Victim Age               0\n",
      "Victim Gender            0\n",
      "Weapon Used           5790\n",
      "Case Closed              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4bb6aa-bb12-4ca5-9f8f-896fffc737ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Weapon Used'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ae3df8-1ef1-4da1-9a43-526009ee243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Number         0\n",
      "Date Reported         0\n",
      "Date of Occurrence    0\n",
      "Time of Occurrence    0\n",
      "City                  0\n",
      "Crime Description     0\n",
      "Victim Age            0\n",
      "Victim Gender         0\n",
      "Weapon Used           0\n",
      "Case Closed           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6339f7-627b-4950-bd5e-6a8d2e95d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the column names\n",
    "df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('-', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8465ebf-1624-48a5-a93a-b00268486024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "df['Date_Reported'] = pd.to_datetime(df['Date_Reported'], errors='coerce')\n",
    "df['Date_of_Occurrence'] = pd.to_datetime(df['Date_of_Occurrence'], errors='coerce')\n",
    "\n",
    "# Convert time to proper format (HH:MM:SS)\n",
    "df['Time_of_Occurrence'] = pd.to_datetime(df['Time_of_Occurrence'], format='%H:%M:%S', errors='coerce').dt.time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aeaeafb-734e-4328-aa32-d363a8391426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging date and time in one column\n",
    "df['DateTime_of_Occurrence'] = pd.to_datetime(df['Date_of_Occurrence'].astype(str) + ' ' +\n",
    "                                              df['Time_of_Occurrence'].astype(str), errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e53d22f7-d6e8-405a-a12e-4ff6f21c8653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Report_Number', 'Date_Reported', 'Date_of_Occurrence',\n",
      "       'Time_of_Occurrence', 'City', 'Crime_Description', 'Victim_Age',\n",
      "       'Victim_Gender', 'Weapon_Used', 'Case_Closed',\n",
      "       'DateTime_of_Occurrence'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d3b0307-2079-400b-839e-6e6d66e74d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop the old date and time columns\n",
    "df.drop(['Date_of_Occurrence', 'Time_of_Occurrence'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a464a7f6-edba-4dae-bfb8-08982e411728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Report_Number', 'Date_Reported', 'City', 'Crime_Description',\n",
      "       'Victim_Age', 'Victim_Gender', 'Weapon_Used', 'Case_Closed',\n",
      "       'DateTime_of_Occurrence'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1d8761-01af-47b3-9d8c-0b2f121ec778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize text (remove extra spaces, make consistent case)\n",
    "df['City'] = df['City'].str.strip().str.title()\n",
    "df['Crime_Description'] = df['Crime_Description'].str.strip().str.upper()\n",
    "df['Victim_Gender'] = df['Victim_Gender'].str.upper()\n",
    "df['Weapon_Used'] = df['Weapon_Used'].str.strip().str.title()\n",
    "df['Case_Closed'] = df['Case_Closed'].str.strip().str.capitalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4498aa71-55de-4f24-bc39-7c4c316bc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert age to numeric\n",
    "df['Victim_Age'] = pd.to_numeric(df['Victim_Age'], errors='coerce')\n",
    "\n",
    "# Handle unrealistic ages (e.g. <0 or >120)\n",
    "df.loc[(df['Victim_Age'] < 0) | (df['Victim_Age'] > 120), 'Victim_Age'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd010f81-f1bb-49fd-bec6-34ebe86dca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Duplicate Values\n",
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6340cb2-b5ff-4184-b01a-a69aa6035ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_cols = ['City', 'Crime_Description', 'Victim_Gender', 'Weapon_Used', 'Case_Closed']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for col in label_cols:\n",
    "    df[col] = encoder.fit_transform(df[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6665bc-ca9f-41c0-812d-ecf2563f205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now saving the cleaned dataset\n",
    "df.to_csv(\"crime_data_cleaned.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1230614-77ff-4c8d-973d-53fe292ffce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure Date is in datetime format\n",
    "df['DateTime_of_Occurrence'] = pd.to_datetime(df['DateTime_of_Occurrence'], errors='coerce')\n",
    "\n",
    "df['Hour'] = df['DateTime_of_Occurrence'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime_of_Occurrence'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "df['Month'] = df['DateTime_of_Occurrence'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c7e6e19-d5cc-4e38-8742-39e7af0c752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_from_time(hour):\n",
    "    if 22 <= hour or hour <= 5:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Low'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88f6cd43-7a35-445a-9f63-92fe6b6793c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk_crimes = ['ASSAULT', 'ROBBERY', 'HOMICIDE', 'RAPE', 'KIDNAPPING', 'BURGLARY']\n",
    "\n",
    "def risk_from_crime(crime):\n",
    "    if pd.isna(crime):\n",
    "        return 'Low'\n",
    "    for keyword in high_risk_crimes:\n",
    "        if keyword.lower() in crime.lower():\n",
    "            return 'High'\n",
    "    return 'Low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b10587b-5900-4ac3-a4be-32041a999fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Report_Number', 'Date_Reported', 'City', 'Crime_Description',\n",
      "       'Victim_Age', 'Victim_Gender', 'Weapon_Used', 'Case_Closed',\n",
      "       'DateTime_of_Occurrence', 'Hour', 'DayOfWeek', 'Month'],\n",
      "      dtype='object')\n",
      "20900\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip()  # removes leading/trailing spaces\n",
    "print(df.columns)\n",
    "type(df)\n",
    "df['Hour'] = pd.to_numeric(df['Hour'], errors='coerce')\n",
    "print(df['Hour'].isnull().sum())  # check how many nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0898af83-b076-4fba-98fc-261a2aa29960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk\n",
      "Low     29843\n",
      "High     4527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def calculate_risk_score(row):\n",
    "    score = 0\n",
    "\n",
    "    # --- Time-based risk ---\n",
    "    time_risk = risk_from_time(row['Hour'])\n",
    "    if time_risk == 'High':\n",
    "        score += 2\n",
    "\n",
    "    # --- Crime-based risk ---\n",
    "    crime_risk = risk_from_crime(str(row['Crime_Description']))\n",
    "    if crime_risk == 'High':\n",
    "        score += 2\n",
    "\n",
    "    # --- Final numeric risk category ---\n",
    "    # If total score >= 2 â†’ High risk, else Low\n",
    "    if score >= 2:\n",
    "        return 2   # High\n",
    "    else:\n",
    "        return 1   # Low\n",
    "\n",
    "\n",
    "# --- Apply to DataFrame ---\n",
    "df.columns = df.columns.str.strip()  # Clean column names\n",
    "df['Hour'] = pd.to_numeric(df['Hour'], errors='coerce')\n",
    "\n",
    "# Apply the function row-wise\n",
    "df['Risk_Score'] = df.apply(calculate_risk_score, axis=1)\n",
    "\n",
    "# Map numeric to categorical labels\n",
    "df['Risk'] = df['Risk_Score'].map({1: 'Low', 2: 'High'})\n",
    "\n",
    "# Check distribution\n",
    "print(df['Risk'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df776d9c-0340-4872-aeb9-2365fcdf2412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imblearn) (0.14.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.3)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb51ad6f-0033-47d7-83b2-a07e0dde32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['City', 'Hour', 'DayOfWeek', 'Month']]\n",
    "y = df['Risk']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07d4ae4d-c215-4fad-996e-796bd43a1540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk\n",
      "Low     29843\n",
      "High     4527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Risk'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83fad5ba-f8f6-4258-a9c1-0b3d3175d1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Report_Number', 'Date_Reported', 'City', 'Crime_Description', 'Victim_Age', 'Victim_Gender', 'Weapon_Used', 'Case_Closed', 'DateTime_of_Occurrence', 'Hour', 'DayOfWeek', 'Month', 'Risk_Score', 'Risk']\n",
      "\n",
      "Class distribution:\n",
      "Risk\n",
      "Low     29843\n",
      "High     4527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63efa6a2-d391-4b8f-b121-decd05ed6ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk\n",
      "Low     29843\n",
      "High     4527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Risk'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9f48d59-dae5-4884-9487-d5d51af11194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34370\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eae7e77f-4d7c-4868-b995-4c7576ecfbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned dataset shape: (13470, 14)\n",
      "DateTime_of_Occurrence    0\n",
      "Hour                      0\n",
      "Month                     0\n",
      "DayOfWeek                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where key time-related columns are missing\n",
    "df = df.dropna(subset=['DateTime_of_Occurrence', 'Hour', 'Month', 'DayOfWeek'])\n",
    "\n",
    "# Optional: Reset the index after dropping rows\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Cleaned dataset shape: {df.shape}\")\n",
    "print(df[['DateTime_of_Occurrence', 'Hour', 'Month', 'DayOfWeek']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3edf4fe1-50b2-4817-b123-5f05ef6fedfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13470\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ceca941-85c9-4b60-ae69-a593d941db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk\n",
      "Low     8943\n",
      "High    4527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Risk'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4150de1-1fdf-43ba-b281-508f436210dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int64(0): np.int64(23874), np.int64(1): np.int64(23874)}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)   \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 4: Handle missing values (SMOTE canâ€™t process NaN)\n",
    "X_train = X_train.fillna(0)\n",
    "\n",
    "# Step 5: Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 6: Check class distribution after balancing\n",
    "unique, counts = np.unique(y_train_res, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a89f2db-3a42-4d4d-a2ff-1d77f3d82864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13470\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d738db8-0438-4df2-96c6-7cf424398cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 27496\n",
      "After SMOTE: 47748\n"
     ]
    }
   ],
   "source": [
    "print(\"Original training size:\", len(X_train))\n",
    "print(\"After SMOTE:\", len(X_train_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "620d6e00-41f1-4d72-82a7-73e7ed94ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.0\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\__init__.py\n",
      "Requirement already satisfied: xgboost in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.0)\n",
      "3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement 1.7.4 (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for 1.7.4\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)\n",
    "print(xgboost.__file__)\n",
    "!pip install xgboost 1.7.4\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "116721fc-f37e-4b53-adeb-7415945efa00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     23\u001b[39m xgb_model_less_overfit = XGBClassifier(\n\u001b[32m     24\u001b[39m     n_estimators=\u001b[32m50\u001b[39m,        \u001b[38;5;66;03m# More trees but weaker (with lower learning rate)\u001b[39;00m\n\u001b[32m     25\u001b[39m     learning_rate=\u001b[32m0.05\u001b[39m,     \u001b[38;5;66;03m# Softer learning rate\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     eval_metric=\u001b[33m'\u001b[39m\u001b[33mlogloss\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     36\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# --- Step 5: Early stopping to prevent overfitting ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mxgb_model\u001b[49m.fit(\n\u001b[32m     41\u001b[39m     X_train, y_train,\n\u001b[32m     42\u001b[39m     eval_set=[(X_test, y_test)],\n\u001b[32m     43\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# --- Step 6: Evaluate model ---\u001b[39;00m\n\u001b[32m     47\u001b[39m y_pred = xgb_model.predict(X_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'xgb_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Step 1: Define features and target ---\n",
    "X = df[['City', 'Hour', 'DayOfWeek', 'Month', 'Crime_Description']]\n",
    "y = df['Risk'].map({'Low': 0, 'High': 1})\n",
    "\n",
    "# --- Step 2: Encode categorical columns ---\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "for col in X_encoded.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- Step 3: Split dataset ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "xgb_model_less_overfit = XGBClassifier(\n",
    "    n_estimators=50,        # More trees but weaker (with lower learning rate)\n",
    "    learning_rate=0.05,     # Softer learning rate\n",
    "    max_depth=2,            # Shallower trees â€” prevents memorization\n",
    "    min_child_weight=12,    # Each leaf needs more samples\n",
    "    subsample=0.5,          # Use only 50% of samples per tree\n",
    "    colsample_bytree=0.5,   # Use only 50% of features per tree\n",
    "    gamma=1,                # Penalize unnecessary splits heavily\n",
    "    reg_lambda=15,          # Very strong L2 regularization\n",
    "    reg_alpha=6,            # Strong L1 regularization\n",
    "    random_state=42,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "\n",
    "# --- Step 5: Early stopping to prevent overfitting ---\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- Step 6: Evaluate model ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"ðŸ”¹ XGBoost Model Evaluation (Regularized) ðŸ”¹\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "878754f9-1847-422c-a07f-ecf7d24c2a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ XGBoost Model Evaluation (Regularized) ðŸ”¹\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[1789    0]\n",
      " [   0  905]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1789\n",
      "           1       1.00      1.00      1.00       905\n",
      "\n",
      "    accuracy                           1.00      2694\n",
      "   macro avg       1.00      1.00      1.00      2694\n",
      "weighted avg       1.00      1.00      1.00      2694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now Trying to Reduce Accuracy as my model is overfitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Step 1: Define features and target ---\n",
    "X = df[['City', 'Hour', 'DayOfWeek', 'Month', 'Crime_Description']]\n",
    "y = df['Risk'].map({'Low': 0, 'High': 1})\n",
    "\n",
    "# --- Step 2: Encode categorical columns ---\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "for col in X_encoded.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- Step 3: Split dataset ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "xgb_model_less_overfit = XGBClassifier(\n",
    "    n_estimators=50,        # More trees but weaker (with lower learning rate)\n",
    "    learning_rate=0.05,     # Softer learning rate\n",
    "    max_depth=2,            # Shallower trees â€” prevents memorization\n",
    "    min_child_weight=7,    # Each leaf needs more samples\n",
    "    subsample=0.6,          # Use only 50% of samples per tree\n",
    "    colsample_bytree=0.5,   # Use only 50% of features per tree\n",
    "    gamma=5,                # Penalize unnecessary splits heavily\n",
    "    reg_lambda=15,          # Very strong L2 regularization\n",
    "    reg_alpha=6,            # Strong L1 regularization\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=10,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "\n",
    "# --- Step 5: Early stopping to prevent overfitting ---\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- Step 6: Evaluate model ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"ðŸ”¹ XGBoost Model Evaluation (Regularized) ðŸ”¹\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9399c5de-7134-429c-bb40-9cb9f49f3ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Random Forest Model Evaluation ðŸŒ²\n",
      "OOB Score: 0.9741091314031181\n",
      "Accuracy: 0.9970304380103935\n",
      "Confusion Matrix:\n",
      " [[1789    0]\n",
      " [   8  897]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1789\n",
      "           1       1.00      0.99      1.00       905\n",
      "\n",
      "    accuracy                           1.00      2694\n",
      "   macro avg       1.00      1.00      1.00      2694\n",
      "weighted avg       1.00      1.00      1.00      2694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Imports ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- Step 2: Define features and target ---\n",
    "X = df[['City', 'Hour', 'DayOfWeek', 'Month', 'Crime_Description']]\n",
    "y = df['Risk'].map({'Low': 0, 'High': 1})  # Encode target variable\n",
    "\n",
    "# --- Step 3: Encode categorical columns ---\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "for col in X_encoded.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- Step 4: Split dataset ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Step 5: Define Random Forest model (with anti-overfitting settings) ---\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=30,        # number of trees\n",
    "    max_depth=3,             # shallow trees to avoid memorization\n",
    "    min_samples_split=30,    # require more samples to split\n",
    "    min_samples_leaf=10,     # require more samples per leaf\n",
    "    max_features=0.5,        # use only 50% of features per tree\n",
    "    bootstrap=True,          # use bootstrapped samples\n",
    "    oob_score=True,          # out-of-bag validation\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # handle imbalance\n",
    ")\n",
    "\n",
    "# --- Step 6: Train model ---\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 7: Evaluate model ---\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"ðŸŒ² Random Forest Model Evaluation ðŸŒ²\")\n",
    "print(\"OOB Score:\", rf_model.oob_score_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e1bf8-64b1-4bff-bce9-477acb0b9c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
